{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3503e7f",
   "metadata": {},
   "source": [
    "# Movielens Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79834da3",
   "metadata": {},
   "source": [
    "## 0. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce12b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.sparse import coo_matrix\n",
    "from implicit.als import AlternatingLeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266880f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "links_df = pd.read_csv(\"data/links.csv\")\n",
    "movies_df = pd.read_csv(\"data/movies.csv\")\n",
    "ratings_df = pd.read_csv(\"data/ratings.csv\")\n",
    "# tags_df = pd.read_csv(\"data/tags.csv\")\n",
    "\n",
    "# Merge ratings with movies on 'movieId'\n",
    "df = pd.merge(ratings_df, movies_df, on='movieId', how='left')\n",
    "\n",
    "# Merge the result with links on 'movieId'\n",
    "df = pd.merge(df, links_df, on='movieId', how='left')\n",
    "\n",
    "# # Merge the result with tags on 'movieId' and 'userId'\n",
    "# df = pd.merge(df, tags_df, on=['movieId', 'userId'], how='left')\n",
    "\n",
    "# # Rename columns for clarity\n",
    "# df.rename(columns={\n",
    "#     'timestamp_x': 'timestamp',\n",
    "#     'timestamp_y': 'timestamp_tag'\n",
    "# }, inplace=True)\n",
    "\n",
    "# Drop N/A's\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "df.to_csv(\"data/merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a05aad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (100823, 8)\n",
      "After cold filtering: (90274, 8)\n",
      "Final sample: (10583, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load the merged DataFrame\n",
    "df = pd.read_csv(\"data/merged.csv\")\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "# Define thresholds\n",
    "MIN_RATINGS_PER_USER  = 5\n",
    "MIN_RATINGS_PER_MOVIE = 5\n",
    "\n",
    "# Discard all users with < MIN_RATINGS_PER_USER\n",
    "df_dense = df.groupby('userId').filter(\n",
    "    lambda user_ratings: len(user_ratings) >= MIN_RATINGS_PER_USER\n",
    ")\n",
    "\n",
    "# Discard all movies with < MIN_RATINGS_PER_MOVIE\n",
    "df_dense = df_dense.groupby('movieId').filter(\n",
    "    lambda movie_ratings: len(movie_ratings) >= MIN_RATINGS_PER_MOVIE\n",
    ")\n",
    "print(\"After cold filtering:\", df_dense.shape)\n",
    "\n",
    "# Define thresholds\n",
    "NUM_USERS   = 500\n",
    "NUM_MOVIES  = 500\n",
    "\n",
    "# Choose a random set of users\n",
    "users = np.random.RandomState(42).choice(\n",
    "    df_dense['userId'].unique(), size=NUM_USERS, replace=False\n",
    ")\n",
    "\n",
    "# Restrict dataframe to those users\n",
    "df_u = df_dense[df_dense['userId'].isin(users)]\n",
    "\n",
    "# Choose a random set of movies from those\n",
    "movies = np.random.RandomState(42).choice(\n",
    "    df_u['movieId'].unique(), size=NUM_MOVIES, replace=False\n",
    ")\n",
    "\n",
    "# Restrict dataframe to those users\n",
    "df_sample = df_u[df_u['movieId'].isin(movies)]\n",
    "print(\"Final sample:\", df_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf451b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 95.76%\n"
     ]
    }
   ],
   "source": [
    "# Check sparsity\n",
    "n_users  = df_sample['userId'].nunique()\n",
    "n_movies = df_sample['movieId'].nunique()\n",
    "n_ratings = len(df_sample)\n",
    "sparsity = 1 - (n_ratings / (n_users * n_movies))\n",
    "print(f\"Sparsity: {sparsity:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550edbd",
   "metadata": {},
   "source": [
    "## 1. Non-Personalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac53e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 by volume: ([318, 480, 2959, 1198, 592, 380, 590, 595, 316, 2329], [264, 199, 181, 167, 151, 149, 137, 124, 114, 110])\n"
     ]
    }
   ],
   "source": [
    "def top_n_count(df, N=10, target_user=None):\n",
    "    # By count of ratings\n",
    "    counts = df['movieId'].value_counts()\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    ids = counts.nlargest(N).index.tolist()\n",
    "    scores = counts.nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "top_count = top_n_count(df_sample)\n",
    "print(\"Top 10 by volume:\", top_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1dfef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 by likes: ([318, 2959, 1198, 480, 2329, 590, 595, 380, 1617, 5418], [228, 150, 136, 125, 90, 84, 69, 65, 61, 60])\n"
     ]
    }
   ],
   "source": [
    "def top_n_likes(df, N=10, threshold=4, target_user=None):\n",
    "    # By count of likes (e.g. rating ≥4)\n",
    "    likes = df[df['rating'] >= threshold]\n",
    "    like_counts = likes['movieId'].value_counts()\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    ids = like_counts.nlargest(N).index.tolist()\n",
    "    scores = like_counts.nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "top_likes = top_n_likes(df_sample)\n",
    "print(\"Top 10 by likes:\",  top_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "483247b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 by average rating: ([56921, 1104, 318, 55721, 1209, 176, 5690, 942, 1280, 2959], [5.0, 4.5625, 4.412878787878788, 4.375, 4.366666666666666, 4.363636363636363, 4.363636363636363, 4.333333333333333, 4.285714285714286, 4.265193370165746])\n"
     ]
    }
   ],
   "source": [
    "def average_rating(df, N=10, target_user=None):\n",
    "    # Calculate average ratings for each movie\n",
    "    average_ratings = df.groupby('movieId')['rating'].mean()\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    ids = average_ratings.nlargest(N).index.tolist()\n",
    "    scores = average_ratings.nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "averages = average_rating(df_sample)\n",
    "print(\"Top 10 by average rating:\", averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be3b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 by normalized average rating: ([56921, 1104, 318, 55721, 1209, 176, 5690, 942, 1280, 2959], [5.0, 4.5625, 4.412878787878788, 4.375, 4.366666666666666, 4.363636363636363, 4.363636363636363, 4.333333333333333, 4.285714285714286, 4.265193370165746])\n"
     ]
    }
   ],
   "source": [
    "def average_rating_normalized(df, N=10, target_user=None):\n",
    "    # Compute user averages\n",
    "    user_avg = df.groupby('userId')['rating'].mean().rename('user_avg')\n",
    "    df = df.merge(user_avg, on='userId')\n",
    "\n",
    "    # Compute predicted score S(u, i) for all (user, item) pairs\n",
    "    predictions = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        u = row['userId']\n",
    "        i = row['movieId']\n",
    "        user_avg_row = row['user_avg']\n",
    "\n",
    "        # Get all ratings for item i\n",
    "        item_ratings = df[df['movieId'] == i]\n",
    "\n",
    "        # Check for cold start problem\n",
    "        if item_ratings.empty:\n",
    "            pred = user_avg_row\n",
    "        else:\n",
    "            deviation_sum = (item_ratings['rating'] - item_ratings['user_avg']).sum()\n",
    "            normalized_score = deviation_sum / len(item_ratings)\n",
    "            pred = user_avg_row + normalized_score\n",
    "\n",
    "        predictions.append((i, pred))\n",
    "\n",
    "    # Create a DataFrame for predictions\n",
    "    pred_df = pd.DataFrame(predictions, columns=['movieId', 'predicted'])\n",
    "\n",
    "    # Calculate normalized average ratings for each movie\n",
    "    average_ratings_normalized = pred_df.groupby('movieId')['predicted'].mean()\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    ids = average_ratings_normalized.nlargest(N).index.tolist()\n",
    "    scores = average_ratings_normalized.nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "normalized_averages = average_rating_normalized(df_sample)\n",
    "print(\"Top 10 by normalized average rating:\", normalized_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032dd95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 by weighted average rating: ([318, 2959, 2329, 1104, 1198, 68157, 1201, 1209, 1617, 68954], [4.380834570799747, 4.226956399995448, 4.1862389366594215, 4.167256630735796, 4.162986849712603, 4.07742283413574, 4.054206653508883, 4.033946895965228, 4.009326070320117, 3.9927630266570073])\n"
     ]
    }
   ],
   "source": [
    "def average_rating_weighted(df, N=10, m=10, target_user=None):\n",
    "    # Compute average rating for each movie (U(j)) and number of votes (v)\n",
    "    average_ratings_weighted = df.groupby('movieId')['rating'].agg(['mean', 'count']).rename(columns={'mean': 'U', 'count': 'v'})\n",
    "\n",
    "    # Compute overall mean rating across all movies (C)\n",
    "    C = df['rating'].mean()\n",
    "\n",
    "    # Compute WR(j) for each movie\n",
    "    average_ratings_weighted['WR'] = (\n",
    "        (average_ratings_weighted['v'] / (average_ratings_weighted['v'] + m)) * average_ratings_weighted['U'] +\n",
    "        (m / (average_ratings_weighted['v'] + m)) * C\n",
    "    )\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    ids = average_ratings_weighted['WR'].nlargest(N).index.tolist()\n",
    "    scores = average_ratings_weighted['WR'].nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "weighted_averages = average_rating_weighted(df_sample)\n",
    "print(\"Top 10 by weighted average rating:\", weighted_averages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf907ed2",
   "metadata": {},
   "source": [
    "## 2. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd0fa44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 user-pearson collaborative filtering: ([8873, 3037, 1273, 37741, 1284, 1292, 57669, 30749, 4033, 265], [2.9330772779188106, 2.9278815241204943, 2.8962773767283827, 2.8798134078740287, 2.8779474510380116, 2.8647858009245306, 2.8638890622035706, 2.863466145556261, 2.8610388522397066, 2.8585657367523143])\n",
      "Top 10 for user 474 user-pearson collaborative filtering: ([50872, 58998, 88163, 96079, 3347, 112175, 1218, 68954, 1280, 942], [3.6430345556346655, 3.589038201421255, 3.5865206542130994, 3.5843209904963773, 3.5707109058564517, 3.5701991324205746, 3.5617317326710918, 3.5562435471559968, 3.5484698631700557, 3.546661304870535])\n",
      "Top 10 for user 448 user-pearson collaborative filtering: ([1219, 2160, 318, 1104, 1273, 910, 2761, 5690, 3037, 1280], [3.5247841133412545, 3.495546211330362, 3.435245931440617, 3.3871915815418685, 3.3489492356408777, 3.3414431801341364, 3.3266106129488726, 3.322713939986797, 3.3108559747180095, 3.2904041622631963])\n"
     ]
    }
   ],
   "source": [
    "def user_based_pearson_cf(df, target_user, N=10, k=30, min_common=3, shrink=10):\n",
    "    # Build user–item matrix\n",
    "    mat = df.pivot_table(\n",
    "        index=\"userId\", columns=\"movieId\", values=\"rating\", aggfunc=\"mean\"\n",
    "    ).astype(float)\n",
    "\n",
    "    # Raise error if the target user isn’t in the matrix\n",
    "    if target_user not in mat.index:\n",
    "        raise ValueError(f\"userId {target_user!r} not found.\")\n",
    "\n",
    "    # Compute user–user pair-wise Pearson similarities\n",
    "    demeaned = mat.sub(mat.mean(axis=1), axis=0)\n",
    "    sim = demeaned.T.corr(method=\"pearson\", min_periods=min_common).fillna(0.0)\n",
    "\n",
    "    # Predict ratings for unrated items\n",
    "    unrated_items = mat.columns[mat.loc[target_user].isna()]\n",
    "    r_u_bar = mat.loc[target_user].mean()\n",
    "    preds = {}\n",
    "\n",
    "    for item in unrated_items:\n",
    "        # Users who have rated this item\n",
    "        neighbours = mat.index[mat[item].notna()]\n",
    "        if neighbours.empty:\n",
    "            continue\n",
    "\n",
    "        # Get similarities\n",
    "        sims = sim.loc[target_user, neighbours]\n",
    "\n",
    "        # Check top k most similar neighbours\n",
    "        top_k = sims.abs().nlargest(k).index\n",
    "        sims_k = sims.loc[top_k]\n",
    "        ratings_k = mat.loc[top_k, item]\n",
    "        means_k = mat.loc[top_k].mean(axis=1)\n",
    "\n",
    "        # Compute the prediction\n",
    "        numer = ((ratings_k - means_k) * sims_k).sum()\n",
    "        denom = sims_k.abs().sum() + shrink\n",
    "\n",
    "        # Save the prediction\n",
    "        preds[item] = r_u_bar + numer / denom if denom else r_u_bar\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    if not preds:\n",
    "        return []\n",
    "    ids = pd.Series(preds).nlargest(N).index.tolist()\n",
    "    scores = pd.Series(preds).nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = user_based_pearson_cf(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} user-pearson collaborative filtering:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390b2176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 user-cosine collaborative filtering: ([590, 3114, 54001, 4246, 57669, 30749, 58998, 5971, 2712, 36], [2.151211079674275, 2.1216005726762144, 2.1072852605736925, 2.089417818690414, 2.046854474286677, 1.958850931593125, 1.9500214180229551, 1.8956318668841179, 1.8880968236240874, 1.8857553590673468])\n",
      "Top 10 for user 474 user-cosine collaborative filtering: ([68954, 50872, 2288, 68157, 63082, 8641, 2916, 51662, 57669, 72998], [2.0597623106720504, 2.0218136249239613, 2.001852902972266, 2.0001903387572226, 1.991285389265404, 1.9696086965175268, 1.9678651002455771, 1.9576384888373273, 1.8403136061179313, 1.8378384003223331])\n",
      "Top 10 for user 448 user-cosine collaborative filtering: ([318, 2329, 1219, 54001, 63082, 595, 4246, 5816, 2288, 34405], [2.4723388100160566, 2.338285607183104, 2.196653027967335, 2.038349337781408, 2.027184775443459, 2.0000461709066695, 1.9889006226637929, 1.9760914947074786, 1.9744752792824494, 1.9707683436127947])\n"
     ]
    }
   ],
   "source": [
    "def user_based_cosine_cf(df, target_user, N=10, k=30, shrink=10):\n",
    "    # Build user–item matrix\n",
    "    mat = df.pivot_table(\n",
    "        index=\"userId\", columns=\"movieId\", values=\"rating\", aggfunc=\"mean\"\n",
    "    ).astype(float)\n",
    "\n",
    "    # Raise error if the target user isn’t in the matrix\n",
    "    if target_user not in mat.index:\n",
    "        raise ValueError(f\"userId {target_user!r} not in data.\")\n",
    "\n",
    "    # Compute user–user cosine similarity on filled mean-centered data\n",
    "    X = mat.fillna(0.0).values\n",
    "    sim = pd.DataFrame(\n",
    "        cosine_similarity(X),\n",
    "        index=mat.index,\n",
    "        columns=mat.index\n",
    "    ).astype(float)\n",
    "\n",
    "    # Predict ratings for unrated items\n",
    "    preds = {}\n",
    "    unrated = mat.columns[mat.loc[target_user].isna()]\n",
    "\n",
    "    for item in unrated:\n",
    "        # Users who have rated this item\n",
    "        neighbours = mat.index[mat[item].notna()]\n",
    "        if neighbours.empty:\n",
    "            continue\n",
    "\n",
    "        # Get similarities\n",
    "        scores = sim.loc[target_user, neighbours]\n",
    "        topk = scores.abs().nlargest(k)\n",
    "        if topk.empty:\n",
    "            continue\n",
    "        \n",
    "        # Compute the prediction\n",
    "        r_k = mat.loc[topk.index, item]\n",
    "        w_k = topk\n",
    "        denom = w_k.abs().sum() + shrink\n",
    "        if denom>0:\n",
    "            preds[item] = (w_k * r_k).sum() / denom\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    if not preds:\n",
    "        return []\n",
    "    ids = pd.Series(preds).nlargest(N).index.tolist()\n",
    "    scores = pd.Series(preds).nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = user_based_cosine_cf(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} user-cosine collaborative filtering:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1fb2336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 item-pearson collaborative filtering: ([56921, 55721, 942, 1177, 1280, 3727, 176, 3675, 1284, 26131], [5.0, 4.411377957432644, 4.336990195619395, 4.334824385461091, 4.324046520293536, 4.284842633332316, 4.201429919648416, 4.1956583414804935, 4.182649478469705, 4.175])\n",
      "Top 10 for user 474 item-pearson collaborative filtering: ([56921, 1280, 942, 55721, 64716, 176, 86, 3727, 1273, 3347], [5.0, 4.352742779612574, 4.348774112952073, 4.325085267368846, 4.318202681099249, 4.2952247099847485, 4.283362006954494, 4.207690186281015, 4.1940931756621955, 4.1393331730247045])\n",
      "Top 10 for user 448 item-pearson collaborative filtering: ([56921, 1104, 5690, 176, 64716, 1280, 942, 55721, 3675, 1131], [5.0, 4.666210304264679, 4.5877214096895305, 4.450772092781765, 4.423323040618656, 4.393861277437147, 4.35999203610821, 4.3408260074708656, 4.2972779262087855, 4.292543011469858])\n"
     ]
    }
   ],
   "source": [
    "def item_based_pearson_cf(df, target_user, N=10, k=30, min_common=3, shrink=10):\n",
    "    # Build user–item matrix\n",
    "    mat = df.pivot_table(\n",
    "        index=\"userId\", columns=\"movieId\", values=\"rating\", aggfunc=\"mean\"\n",
    "    ).astype(float)\n",
    "\n",
    "    # Raise error if the target user isn’t in the matrix\n",
    "    if target_user not in mat.index:\n",
    "        raise ValueError(f\"userId {target_user!r} not found.\")\n",
    "\n",
    "    # Compute user–user pair-wise Pearson similarities\n",
    "    item_means = mat.mean(axis=0)\n",
    "    demeaned = mat.sub(item_means, axis=1)\n",
    "    sim = demeaned.corr(method=\"pearson\", min_periods=min_common).fillna(0.0)\n",
    "\n",
    "    # Predict ratings for unrated items\n",
    "    preds = {}\n",
    "    seen = mat.loc[target_user].dropna().index\n",
    "    candidates = mat.columns[mat.loc[target_user].isna()]\n",
    "\n",
    "    for item in candidates:\n",
    "        # Check if the item exists in similarity matrix\n",
    "        if item not in sim.index:\n",
    "            continue\n",
    "\n",
    "        # Users who have rated this item\n",
    "        neighbours = seen.intersection(sim.index)\n",
    "        if neighbours.empty:\n",
    "            continue\n",
    "\n",
    "        # Check top k most similar neighbours\n",
    "        sims = sim.loc[item, neighbours]\n",
    "        top_k = sims.abs().nlargest(k).index\n",
    "        sims_k = sims.loc[top_k]\n",
    "\n",
    "        # Compute the prediction\n",
    "        r_k = mat.loc[target_user, top_k]\n",
    "        m_k = item_means[top_k]\n",
    "        numer = ((r_k - m_k) * sims_k).sum()\n",
    "        denom = sims_k.abs().sum() + shrink\n",
    "\n",
    "        # Save the prediction\n",
    "        preds[item] = item_means[item] + (numer / denom if denom else 0.0)\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    if not preds:\n",
    "        return []\n",
    "    ids = pd.Series(preds).nlargest(N).index.tolist()\n",
    "    scores = pd.Series(preds).nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = item_based_pearson_cf(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} item-pearson collaborative filtering:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823d0eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 item-cosine collaborative filtering: ([54001, 57669, 5299, 590, 3114, 8376, 4246, 30749, 100163, 71254], [1.6642694750768543, 1.647378946949287, 1.6388093363047247, 1.6239976969605672, 1.6203418819026274, 1.6189647355083898, 1.577606014196386, 1.5532642525137776, 1.5511647389846237, 1.5412309576633139])\n",
      "Top 10 for user 474 item-cosine collaborative filtering: ([51662, 34162, 8641, 2916, 50872, 63082, 72998, 68954, 1391, 68157], [2.1416924833491886, 2.059880415828726, 2.019286116767599, 2.0086108981427495, 2.001171037360891, 1.9689186320279444, 1.947360892119922, 1.9248969723524472, 1.9232270589073828, 1.9023300824742582])\n",
      "Top 10 for user 448 item-cosine collaborative filtering: ([318, 1219, 5816, 2329, 920, 4246, 54001, 595, 2712, 63082], [2.0106448558043075, 1.9760723396665953, 1.966469578618982, 1.9510499072779741, 1.8494640344197988, 1.847628252693416, 1.837478900947019, 1.8331124753965806, 1.8314845234335542, 1.8271448459033282])\n"
     ]
    }
   ],
   "source": [
    "def item_based_cosine_cf(df, target_user, N=10, k=30, shrink=10):\n",
    "    # Build user–item matrix\n",
    "    mat = df.pivot_table(\n",
    "        index=\"userId\", columns=\"movieId\", values=\"rating\", aggfunc=\"mean\"\n",
    "    ).astype(float)\n",
    "\n",
    "    # Raise error if the target user isn’t in the matrix\n",
    "    if target_user not in mat.index:\n",
    "        raise ValueError(f\"userId {target_user!r} not in data.\")\n",
    "\n",
    "    # Compute user–user cosine similarity on filled mean-centered data\n",
    "    X = mat.fillna(0.0).T.values\n",
    "    sim = pd.DataFrame(\n",
    "        cosine_similarity(X),\n",
    "        index=mat.columns,\n",
    "        columns=mat.columns\n",
    "    ).astype(float)\n",
    "\n",
    "    # Collect the user’s existing ratings\n",
    "    user_ratings = mat.loc[target_user].dropna()\n",
    "    if user_ratings.empty:\n",
    "        return []\n",
    "\n",
    "    # Predict ratings for unrated items\n",
    "    preds = {}\n",
    "    candidates = mat.columns[mat.loc[target_user].isna()]\n",
    "    \n",
    "    for item in candidates:\n",
    "        # Get similarities\n",
    "        scores = sim.loc[item, user_ratings.index]\n",
    "        \n",
    "        # Pick top k most similar neighbours\n",
    "        topk = scores.abs().nlargest(k)\n",
    "        if topk.empty:\n",
    "            continue\n",
    "\n",
    "        # Compute the prediction\n",
    "        r_j = user_ratings[topk.index]\n",
    "        w_j = topk\n",
    "        denom = w_j.abs().sum() + shrink\n",
    "        if denom > 0:\n",
    "            preds[item] = (w_j * r_j).sum() / denom\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    if not preds:\n",
    "        return []\n",
    "    ids = pd.Series(preds).nlargest(N).index.tolist()\n",
    "    scores = pd.Series(preds).nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = item_based_cosine_cf(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} item-cosine collaborative filtering:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50227cd",
   "metadata": {},
   "source": [
    "## 3. Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71074952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 content-cosine filtering: ([171, 1734, 2926, 2374, 3565, 2431, 1292, 3506, 2469, 57669], [0.7032472819670628, 0.7032472819670628, 0.7032472819670628, 0.7032472819670628, 0.7032472819670628, 0.7032472819670628, 0.7032472819670628, 0.7032472819670628, 0.7032472819670628, 0.6965647155175801])\n",
      "Top 10 for user 474 content-cosine filtering: ([171, 1734, 2926, 3565, 45440, 3506, 71464, 32291, 8529, 60950], [0.7738129901920858, 0.7738129901920858, 0.7738129901920858, 0.7738129901920858, 0.7738129901920858, 0.7738129901920858, 0.7738129901920858, 0.7738129901920858, 0.7364707038361644, 0.7364707038361644])\n",
      "Top 10 for user 448 content-cosine filtering: ([5628, 171, 345, 1734, 2926, 371, 2374, 3565, 1810, 2431], [0.774533181110697, 0.7089108535014519, 0.7089108535014519, 0.7089108535014519, 0.7089108535014519, 0.7089108535014519, 0.7089108535014519, 0.7089108535014519, 0.7089108535014519, 0.7089108535014519])\n"
     ]
    }
   ],
   "source": [
    "def content_based_cosine_f(df, target_user, N=10):\n",
    "    # Build movie-feature matrix from genres\n",
    "    movies = (\n",
    "        df[['movieId','genres']]\n",
    "        .drop_duplicates()\n",
    "        .assign(genres_list=lambda d: d['genres'].str.split('|'))\n",
    "    )\n",
    "    \n",
    "    # Encode genres as binary features\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genre_mat = mlb.fit_transform(movies['genres_list'])\n",
    "\n",
    "    # Apply TF-IDF transformation\n",
    "    tfidf = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "    tfidf_mat = tfidf.fit_transform(genre_mat)\n",
    "\n",
    "    # Create DataFrame with movie features\n",
    "    features = pd.DataFrame(\n",
    "        tfidf_mat.toarray(),\n",
    "        index=movies['movieId'],\n",
    "        columns=mlb.classes_\n",
    "    )\n",
    "\n",
    "    # Build user profile as weighted average of their rated-movie features\n",
    "    user_hist = (\n",
    "        df[df['userId'] == target_user]\n",
    "          .loc[:, ['movieId','rating']]\n",
    "          .dropna(subset=['rating'])\n",
    "    )\n",
    "    if user_hist.empty:\n",
    "        return []\n",
    "\n",
    "    # Align features & ratings\n",
    "    user_feats = features.loc[user_hist['movieId']].values\n",
    "    ratings = user_hist['rating'].values.reshape(-1,1)\n",
    "\n",
    "    # Weighted average as singular profile vector\n",
    "    user_profile = (user_feats * ratings).sum(axis=0) / ratings.sum()\n",
    "\n",
    "    # Score all unseen movies\n",
    "    seen = set(user_hist['movieId'])\n",
    "    candidates = [m for m in features.index if m not in seen]\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cand_feat = features.loc[candidates].values\n",
    "    sims = cosine_similarity(cand_feat, user_profile.reshape(1,-1)).flatten()\n",
    "    sims_series = pd.Series(sims, index=candidates)\n",
    "    \n",
    "    # Return top N movieIds and scores\n",
    "    if sims_series.empty:\n",
    "        return []\n",
    "    ids = sims_series.nlargest(N).index.tolist()\n",
    "    scores = sims_series.nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = content_based_cosine_f(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} content-cosine filtering:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f9741",
   "metadata": {},
   "source": [
    "## 4. Matrix Factorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaa24ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 SVD matrix factorisation: ([36, 8376, 158238, 85780, 5299, 58998, 1273, 3101, 135887, 1209], [3.282265291160939, 3.107580173886322, 2.9714740359334533, 2.946639824379959, 2.9461875345683652, 2.9223610716231545, 2.919973436955726, 2.9199166676819868, 2.914709773036582, 2.9040818547510185])\n",
      "Top 10 for user 474 SVD matrix factorisation: ([68319, 145, 2720, 2916, 1805, 432, 72998, 89904, 1370, 45662], [3.706906045971289, 3.665372201287005, 3.6543426950746696, 3.65060924563688, 3.625348077251128, 3.6191021257611764, 3.617514592185122, 3.6025255185745277, 3.572679384833827, 3.5480552649327115])\n",
      "Top 10 for user 448 SVD matrix factorisation: ([2761, 1499, 30749, 1094, 1049, 56782, 7254, 69, 52, 2080], [3.318459603432919, 3.3156189970986922, 3.293660858921625, 3.290801225391084, 3.251077493582337, 3.2489460699071766, 3.2408184325359217, 3.238063748636572, 3.2364996498544603, 3.2355534287986223])\n"
     ]
    }
   ],
   "source": [
    "def matrix_factorisation_svd(df, target_user, N=10, k=30, random_state=42):\n",
    "    # Build user–item matrix\n",
    "    R = (\n",
    "        df.pivot_table(\n",
    "            index=\"userId\", columns=\"movieId\", values=\"rating\", aggfunc=\"mean\"\n",
    "        )\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    # Raise error if the target user isn’t in the matrix\n",
    "    if target_user not in R.index:\n",
    "        raise ValueError(f\"userId {target_user!r} not found in data.\")\n",
    "\n",
    "    # Demean by user means\n",
    "    user_means = R.mean(axis=1)\n",
    "    R_demeaned = R.sub(user_means, axis=0).fillna(0.0)\n",
    "\n",
    "    # Apply Model\n",
    "    model = TruncatedSVD(n_components=k, random_state=random_state)\n",
    "    user_factors = model.fit_transform(R_demeaned.values)\n",
    "    item_factors = model.components_\n",
    "\n",
    "    # Approximate ratings and add back means\n",
    "    R_hat = np.dot(user_factors, item_factors)\n",
    "    R_hat += user_means.values.reshape(-1, 1)\n",
    "\n",
    "    # Save predictions\n",
    "    preds_df = pd.DataFrame(R_hat, index=R.index, columns=R.columns)\n",
    "\n",
    "    # Pick top N for the target user\n",
    "    user_pred = preds_df.loc[target_user]\n",
    "    seen = R.loc[target_user].dropna().index\n",
    "    top_by_matrix_factorisation_svd = (\n",
    "        user_pred.drop(seen).nlargest(N).index.tolist()\n",
    "    )\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    if not top_by_matrix_factorisation_svd:\n",
    "        return []\n",
    "    ids = top_by_matrix_factorisation_svd\n",
    "    scores = user_pred[top_by_matrix_factorisation_svd].values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = matrix_factorisation_svd(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} SVD matrix factorisation:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d54bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 NMF matrix factorisation: ([8376, 4448, 57669, 2944, 1209, 4246, 54001, 8781, 2712, 2139], [2.71569923831463, 2.5093419892325084, 2.4125014902415245, 2.409982495305774, 2.3987092084288633, 2.2792379749161853, 2.105383963041863, 2.0922070714483354, 2.064199731476649, 1.9559231099035692])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 474 NMF matrix factorisation: ([32291, 98124, 4275, 176371, 47423, 6659, 37857, 719, 45440, 34162], [1.018492376650457, 0.9337489949401748, 0.9236707448682046, 0.9217502803673463, 0.9048629311314674, 0.9036430411911786, 0.9004096859508648, 0.8514016580878903, 0.8354584223514891, 0.831252463391934])\n",
      "Top 10 for user 448 NMF matrix factorisation: ([58998, 3100, 2329, 1805, 2193, 1257, 4008, 4041, 1094, 2761], [3.3043630074406436, 3.2289566327039494, 2.3112305349762154, 2.2460252100402993, 2.196552001977241, 2.100459887183775, 2.0380718913627813, 1.900379984976353, 1.875305711458508, 1.8187769677547803])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def matrix_factorisation_nmf(df, target_user, N=10, k=30, random_state=42, max_iter=500):\n",
    "    # Build user–item matrix\n",
    "    R = (\n",
    "        df.pivot_table(\n",
    "            index=\"userId\", columns=\"movieId\", values=\"rating\", aggfunc=\"mean\"\n",
    "        )\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    # Raise error if the target user isn’t in the matrix\n",
    "    if target_user not in R.index:\n",
    "        raise ValueError(f\"userId {target_user!r} not found in data.\")\n",
    "\n",
    "    # Apply Model\n",
    "    model = NMF(n_components=k, init=\"random\", random_state=random_state, max_iter=max_iter, tol=1e-4)\n",
    "    user_factors = model.fit_transform(R.values)\n",
    "    item_factors = model.components_\n",
    "\n",
    "    # Reconstruct ratings matrix\n",
    "    R_hat = np.dot(user_factors, item_factors)\n",
    "\n",
    "    # Save predictions\n",
    "    preds_df = pd.DataFrame(R_hat, index=R.index, columns=R.columns)\n",
    "\n",
    "    # Pick top N for the target user\n",
    "    user_pred = preds_df.loc[target_user]\n",
    "    seen = R.loc[target_user].to_numpy().nonzero()[0]\n",
    "    seen_ids = R.loc[target_user][R.loc[target_user] > 0].index\n",
    "    top_by_matrix_factorisation_nmf = (\n",
    "        user_pred.drop(seen_ids).nlargest(N).index.tolist()\n",
    "    )\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    if not top_by_matrix_factorisation_nmf:\n",
    "        return []\n",
    "    ids = top_by_matrix_factorisation_nmf\n",
    "    scores = user_pred[top_by_matrix_factorisation_nmf].values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = matrix_factorisation_nmf(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} NMF matrix factorisation:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6234bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 4 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "100%|██████████| 15/15 [00:00<00:00, 129.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 ALS matrix factorisation: ([79185, 113378, 590, 611, 1202, 2720, 4448, 2944, 118900, 3249], [0.3258478343486786, 0.2731245756149292, 0.22364316880702972, 0.16128499805927277, 0.1462622582912445, 0.144228994846344, 0.13965092599391937, 0.13898852467536926, 0.13785482943058014, 0.13633930683135986])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 130.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 474 ALS matrix factorisation: ([7845, 2926, 37857, 5009, 51077, 89774, 688, 68157, 111113, 145], [0.875626266002655, 0.5513898134231567, 0.41076260805130005, 0.4005529582500458, 0.3783547282218933, 0.36520931124687195, 0.35562241077423096, 0.35036346316337585, 0.2767240107059479, 0.26729315519332886])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 131.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 448 ALS matrix factorisation: ([3469, 2245, 1284, 2390, 920, 96610, 1359, 2937, 42, 2208], [0.8502200841903687, 0.8072934746742249, 0.6354320645332336, 0.5123622417449951, 0.4474309980869293, 0.40943294763565063, 0.40674248337745667, 0.4015004634857178, 0.38561147451400757, 0.3722541630268097])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def matrix_factorisation_als(df, target_user, N=10, factors=30, regularization=0.1, iterations=15, alpha=1.0):\n",
    "    # Build id mappings\n",
    "    unique_users = df['userId'].unique()\n",
    "    unique_items = df['movieId'].unique()\n",
    "    user2idx = {u: i for i, u in enumerate(unique_users)}\n",
    "    item2idx = {m: i for i, m in enumerate(unique_items)}\n",
    "    idx2item = {i: m for m, i in item2idx.items()}\n",
    "\n",
    "    # Raise error if the target user isn’t in the matrix\n",
    "    if target_user not in user2idx:\n",
    "        raise ValueError(f\"userId {target_user!r} not found in data.\")\n",
    "\n",
    "    # Build item-user confidence matrix\n",
    "    rows = df['movieId'].map(item2idx).to_numpy()\n",
    "    cols = df['userId'].map(user2idx).to_numpy()\n",
    "    data = (1.0 + alpha * df['rating'].astype(float)).to_numpy()\n",
    "    item_user_csr = coo_matrix(\n",
    "        (data, (rows, cols)),\n",
    "        shape=(len(unique_items), len(unique_users))\n",
    "    ).tocsr()\n",
    "\n",
    "    # Apply model\n",
    "    model = AlternatingLeastSquares(\n",
    "        factors=factors,\n",
    "        regularization=regularization,\n",
    "        iterations=iterations,\n",
    "        calculate_training_loss=False\n",
    "    )\n",
    "    model.fit(item_user_csr)\n",
    "\n",
    "    # Get raw scores\n",
    "    uidx = user2idx[target_user]\n",
    "    user_vec = model.user_factors[uidx]\n",
    "    item_vecs = model.item_factors\n",
    "    scores_all = item_vecs.dot(user_vec)\n",
    "\n",
    "    # Mask out already-rated items   \n",
    "    seen_items = df.loc[df['userId'] == target_user, 'movieId'].unique()\n",
    "    seen_idx = [item2idx[m] for m in seen_items if m in item2idx]\n",
    "    for i in seen_idx:\n",
    "        if 0 <= i < scores_all.shape[0]:\n",
    "            scores_all[i] = -np.inf\n",
    "\n",
    "    # Pick Top N\n",
    "    top_idx = np.argpartition(-scores_all, N)[:N]\n",
    "    top_idx = top_idx[np.argsort(-scores_all[top_idx])]\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    ids = [idx2item[i] for i in top_idx]\n",
    "    scores = scores_all[top_idx].tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = matrix_factorisation_als(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} ALS matrix factorisation:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75df3e",
   "metadata": {},
   "source": [
    "## 5. Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a64ccfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z normalization\n",
    "def _zscore(s: pd.Series) -> pd.Series:\n",
    "    return (s - s.mean()) / (s.std(ddof=0) + 1e-9)\n",
    "\n",
    "# Min-Max normalization\n",
    "def _minmax01(s: pd.Series) -> pd.Series:\n",
    "    rng = s.max() - s.min()\n",
    "    return (s - s.min()) / (rng + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71313c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 hybrid collaborative filtering & avg. weighted recommender: ([54001, 590, 3114, 57669, 4246, 30749, 5971, 58998, 36, 55247], [0.9999999997528908, 0.9955276320390531, 0.985216444213922, 0.9694779495811998, 0.9426606995665555, 0.907980097689071, 0.9011205141597508, 0.8959691155170222, 0.8734377184638255, 0.8398130154433585])\n",
      "Top 10 for user 474 hybrid collaborative filtering & avg. weighted recommender: ([68954, 68157, 50872, 2288, 63082, 8641, 2916, 57669, 51662, 72998], [0.9999999997516398, 0.9902203888405727, 0.9662082209061752, 0.9471296192304157, 0.9318292646940572, 0.9188857240010104, 0.8961714287289245, 0.8919081103638996, 0.8907411838895203, 0.8434340979479239])\n",
      "Top 10 for user 448 hybrid collaborative filtering & avg. weighted recommender: ([318, 2329, 1219, 54001, 63082, 595, 2288, 34405, 910, 2761], [0.9999999997906448, 0.9261191436281038, 0.8488071190428258, 0.7882645791332755, 0.7604490908960007, 0.7522426783144225, 0.7519766752608774, 0.7455249379704997, 0.7426385006076288, 0.7399427388190982])\n"
     ]
    }
   ],
   "source": [
    "def hybrid_recommender_cf_aw(df, target_user, N=10, alpha=0.8, min_interactions=3, k=30, shrink=10, m=10):\n",
    "    # Collect user history\n",
    "    interactions = df[df[\"userId\"] == target_user]\n",
    "    seen_items: set[int] = set(interactions[\"movieId\"].unique())\n",
    "    seen_count = len(seen_items)\n",
    "\n",
    "    # Popularity prior – Bayesian weighted rating for every movie\n",
    "    n_movies = df[\"movieId\"].nunique()\n",
    "    prior_ids, prior_scores = average_rating_weighted(df, N=n_movies, m=m)\n",
    "    prior_series = pd.Series(prior_scores, index=prior_ids, name=\"prior\")\n",
    "\n",
    "    # Cold‑start path – rely entirely on the prior  \n",
    "    if seen_count < min_interactions:\n",
    "        top = prior_series[~prior_series.index.isin(seen_items)].nlargest(N)\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"movieId\": top.index,\n",
    "                \"score\": _minmax01(top.values),\n",
    "                \"cf_raw\": np.nan,\n",
    "                \"prior_raw\": top.values,\n",
    "                \"alpha_used\": 0.0,\n",
    "                \"seen_count\": seen_count,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Personalised CF scores  \n",
    "    cf_ids, cf_scores = user_based_cosine_cf(\n",
    "        df,\n",
    "        target_user,\n",
    "        N=max(1000, n_movies),\n",
    "        k=k,\n",
    "        shrink=shrink,\n",
    "    )\n",
    "    cf_series = pd.Series(cf_scores, index=cf_ids, name=\"cf\")\n",
    "\n",
    "    # Blend the two models\n",
    "    candidates = prior_series.index.union(cf_series.index).difference(seen_items)\n",
    "    cf_aligned = cf_series.reindex(candidates)\n",
    "    prior_aligned = prior_series.reindex(candidates)\n",
    "\n",
    "    # Normalise to share scale before mixing\n",
    "    cf_norm = _zscore(cf_aligned.fillna(cf_aligned.mean()))\n",
    "    prior_norm = _zscore(prior_aligned)\n",
    "\n",
    "    # Mix the two models\n",
    "    final = alpha * cf_norm + (1 - alpha) * prior_norm\n",
    "    final = _minmax01(final)\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    ids = final.nlargest(N).index.tolist()\n",
    "    scores = final.nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = hybrid_recommender_cf_aw(df_sample, target_user=user)\n",
    "    print(f\"Top 10 for user {user} hybrid collaborative filtering & avg. weighted recommender:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bba534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 for user 599 hybrid dynamic recommender: ([54001, 57669, 590, 3114, 5299, 8376, 4246, 30749, 58998, 71254], [0.9999999997506893, 0.9823387113646082, 0.9449131360730677, 0.9432169491996957, 0.9191060133212328, 0.8999491411466705, 0.887026415460161, 0.8729325451947673, 0.8405997531116943, 0.8285963807767025])\n",
      "Top 10 for user 474 hybrid dynamic recommender: ([51662, 34162, 8641, 50872, 2916, 63082, 68954, 68157, 72998, 8873], [0.9999999997779269, 0.9363152836191435, 0.9238612857046477, 0.9219221938432781, 0.9054808072867694, 0.8890070277876002, 0.8740077904367726, 0.8650392080002691, 0.8624987614596469, 0.823127722936592])\n",
      "Top 10 for user 448 hybrid dynamic recommender: ([318, 1219, 2329, 5816, 54001, 4246, 595, 920, 63082, 2944], [0.9999999997783844, 0.9491770069566781, 0.94458473570229, 0.9165068576064693, 0.8457133373119707, 0.8296251636238859, 0.826785585757196, 0.8248208666770616, 0.8234808440503364, 0.8063041193445082])\n"
     ]
    }
   ],
   "source": [
    "def hybrid_recommender(df, target_user, cold_model=average_rating_weighted, warm_model=user_based_cosine_cf, N=10, alpha=0.8, min_interactions=3):\n",
    "    # Collect user history\n",
    "    interactions = df[df[\"userId\"] == target_user]\n",
    "    seen_items: set[int] = set(interactions[\"movieId\"].unique())\n",
    "    seen_count = len(seen_items)\n",
    "\n",
    "    # Apply cold model\n",
    "    n_movies = df[\"movieId\"].nunique()\n",
    "    prior_ids, prior_scores = cold_model(df, N=n_movies)\n",
    "    prior_series = pd.Series(prior_scores, index=prior_ids, name=\"prior\")\n",
    "\n",
    "    # Cold‑start path – rely entirely on the prior  \n",
    "    if seen_count < min_interactions:\n",
    "        print(f\"Cold Start Detected: User {target_user} has only {seen_count} interactions, using prior model.\")\n",
    "        top = prior_series[~prior_series.index.isin(seen_items)].nlargest(N)\n",
    "\n",
    "        # Return top N movieIds and scores\n",
    "        ids = top.index.tolist()\n",
    "        scores = top.values.tolist()\n",
    "        return ids, scores\n",
    "\n",
    "    # Apply warm model\n",
    "    cf_ids, cf_scores = warm_model(df, target_user, N=max(1000, n_movies))\n",
    "    cf_series = pd.Series(cf_scores, index=cf_ids, name=\"cf\")\n",
    "\n",
    "    # Blend the two models\n",
    "    candidates = prior_series.index.union(cf_series.index).difference(seen_items)\n",
    "    cf_aligned = cf_series.reindex(candidates)\n",
    "    prior_aligned = prior_series.reindex(candidates)\n",
    "\n",
    "    # Normalise to share scale before mixing\n",
    "    cf_norm = _zscore(cf_aligned.fillna(cf_aligned.mean()))\n",
    "    prior_norm = _zscore(prior_aligned)\n",
    "\n",
    "    # Mix the two models\n",
    "    final = alpha * cf_norm + (1 - alpha) * prior_norm\n",
    "    final = _minmax01(final)\n",
    "\n",
    "    # Return top N movieIds and scores\n",
    "    ids = final.nlargest(N).index.tolist()\n",
    "    scores = final.nlargest(N).values.tolist()\n",
    "    return ids, scores\n",
    "\n",
    "\n",
    "# Testing\n",
    "# Get recommendations for the top 3 most popular users\n",
    "top_users = df_sample['userId'].value_counts().nlargest(3).index.tolist()\n",
    "for user in top_users:\n",
    "    cf_recommendations = hybrid_recommender(df_sample, target_user=user, cold_model=average_rating, warm_model=item_based_cosine_cf)\n",
    "    print(f\"Top 10 for user {user} hybrid dynamic recommender:\", cf_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c0c64",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17574f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train-test split function\n",
    "def make_train_test(df, test_size=0.2):\n",
    "    # Hold out the most recent X% of interactions\n",
    "    df_sorted = df.sort_values(\"timestamp\")\n",
    "    cutoff = int(len(df) * (1 - test_size))\n",
    "    train = df_sorted.iloc[:cutoff]\n",
    "    test  = df_sorted.iloc[cutoff:]\n",
    "    return train.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "### Top N evaluation function\n",
    "def evaluate_top_n(df_train, df_test, model_func, N=10):\n",
    "    # Build ground‐truth list\n",
    "    gt = df_test.groupby('userId')['movieId'].apply(list).to_dict()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions, recalls, mrrs = [], [], []\n",
    "    train_users = set(df_train['userId'].unique())\n",
    "    \n",
    "    # Process each user in the test set\n",
    "    for user, actual in gt.items():\n",
    "        if not actual or user not in train_users:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Get recommendations\n",
    "            recs_and_scores = model_func(df_train, target_user=user, N=N)\n",
    "            \n",
    "            # Handle both return types (older functions might just return IDs)\n",
    "            if isinstance(recs_and_scores, tuple) and len(recs_and_scores) == 2:\n",
    "                recs, scores = recs_and_scores\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            if not recs:\n",
    "                continue\n",
    "                \n",
    "            # Calculate ranking metrics\n",
    "            precisions.append(len(set(recs[:N]) & set(actual)) / N)\n",
    "            recalls.append(len(set(recs[:N]) & set(actual)) / len(actual))\n",
    "            \n",
    "            # MRR calculation\n",
    "            rank = next((i+1 for i, r in enumerate(recs[:N]) if r in actual), 0)\n",
    "            mrrs.append(1/rank if rank > 0 else 0)\n",
    "            \n",
    "        except Exception:\n",
    "            # Skip problematic users\n",
    "            continue\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return {\n",
    "        f'Precision@{N}': np.nanmean(precisions) if precisions else np.nan,\n",
    "        f'Recall@{N}': np.nanmean(recalls) if recalls else np.nan,\n",
    "        f'MRR@{N}': np.nanmean(mrrs) if mrrs else np.nan,\n",
    "    }\n",
    "\n",
    "\n",
    "### RMSE evaluation function\n",
    "def evaluate_rmse(df_train, df_test, model_func, N=10):\n",
    "    # Create a dictionary mapping (userId, movieId) to rating\n",
    "    test_ratings = df_test.set_index(['userId', 'movieId'])['rating'].to_dict()\n",
    "    \n",
    "    # Initialize a list to collect errors\n",
    "    all_errors = []\n",
    "    train_users = set(df_train['userId'].unique())\n",
    "    \n",
    "    # Process each user in the test set\n",
    "    for user in df_test['userId'].unique():\n",
    "        if user not in train_users:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Get recommendations and scores\n",
    "            recs_and_scores = model_func(df_train, target_user=user, N=N)\n",
    "            \n",
    "            # Handle different return types\n",
    "            if isinstance(recs_and_scores, tuple) and len(recs_and_scores) == 2:\n",
    "                recs, scores = recs_and_scores\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            if not recs:\n",
    "                continue\n",
    "                \n",
    "            # Find items that exist in both recommendations and test set\n",
    "            user_errors = []\n",
    "            for i, item in enumerate(recs):\n",
    "                if (user, item) in test_ratings:\n",
    "                    actual = test_ratings[(user, item)]\n",
    "                    predicted = scores[i]  \n",
    "                    user_errors.append((actual - predicted) ** 2)\n",
    "            \n",
    "            if user_errors:  # Only add if we have overlapping items\n",
    "                all_errors.extend(user_errors)\n",
    "                \n",
    "        except Exception:\n",
    "            # Skip problematic users\n",
    "            continue\n",
    "    \n",
    "    # Return RMSE\n",
    "    return np.sqrt(np.mean(all_errors)) if all_errors else np.nan\n",
    "\n",
    "\n",
    "### Model evaluation function\n",
    "def evaluate_models(df_train, df_test, models, N=10):\n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Iterate over each model\n",
    "    for model_name, model_func in models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Evaluate top-N metrics\n",
    "            topn_metrics = evaluate_top_n(df_train, df_test, model_func, N=N)\n",
    "            \n",
    "            # Evaluate RMSE (if applicable)\n",
    "            rmse = evaluate_rmse(df_train, df_test, model_func, N=N)\n",
    "            \n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                'TopN': topn_metrics,\n",
    "                'RMSE': rmse\n",
    "            }\n",
    "            \n",
    "            # Display results\n",
    "            metrics_str = ', '.join([f\"{k}: {v:.4f}\" for k, v in topn_metrics.items()])\n",
    "            print(f\"{model_name} - {metrics_str}, RMSE: {rmse:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {model_name}: {e}\")\n",
    "            results[model_name] = {'error': str(e)}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a5e6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    # Non-personalized models\n",
    "    'Top N Count': top_n_count,\n",
    "    'Top N Likes': top_n_likes,\n",
    "    'Average Rating': average_rating,\n",
    "    'Average Rating Normalized': average_rating_normalized,\n",
    "    'Average Rating Weighted': average_rating_weighted,\n",
    "    # Personalized models\n",
    "    'User CF Pearson': user_based_pearson_cf,\n",
    "    'User CF Cosine': user_based_cosine_cf,\n",
    "    'Item CF Pearson': item_based_pearson_cf,\n",
    "    'Item CF Cosine': item_based_cosine_cf,\n",
    "    'Content CF Cosine': content_based_cosine_f,\n",
    "    'Matrix Factorisation SVD': matrix_factorisation_svd,\n",
    "    'Matrix Factorisation NMF': matrix_factorisation_nmf,\n",
    "    'Matrix Factorisation ALS': matrix_factorisation_als,\n",
    "    'Hybrid CF & Avg Weighted': hybrid_recommender_cf_aw\n",
    "}\n",
    "\n",
    "# Split data into train and test sets\n",
    "df_train, df_test = make_train_test(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40e63fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Top N Count...\n",
      "Top N Count - Precision@10: 0.0357, Recall@10: 0.1081, MRR@10: 0.1811, RMSE: 160.6179\n",
      "Evaluating Top N Likes...\n",
      "Top N Likes - Precision@10: 0.0429, Recall@10: 0.1111, MRR@10: 0.1994, RMSE: 119.8845\n",
      "Evaluating Average Rating...\n",
      "Average Rating - Precision@10: 0.0286, Recall@10: 0.0985, MRR@10: 0.0400, RMSE: 1.6000\n",
      "Evaluating Average Rating Normalized...\n",
      "Average Rating Normalized - Precision@10: 0.0286, Recall@10: 0.0985, MRR@10: 0.0400, RMSE: 1.6000\n",
      "Evaluating Average Rating Weighted...\n",
      "Average Rating Weighted - Precision@10: 0.0643, Recall@10: 0.1233, MRR@10: 0.2024, RMSE: 0.3744\n",
      "Evaluating User CF Pearson...\n",
      "User CF Pearson - Precision@10: 0.0571, Recall@10: 0.1109, MRR@10: 0.2381, RMSE: 1.6648\n",
      "Evaluating User CF Cosine...\n",
      "User CF Cosine - Precision@10: 0.1214, Recall@10: 0.1860, MRR@10: 0.4054, RMSE: 2.4447\n",
      "Evaluating Item CF Pearson...\n",
      "Item CF Pearson - Precision@10: 0.0214, Recall@10: 0.0896, MRR@10: 0.0312, RMSE: 1.7690\n",
      "Evaluating Item CF Cosine...\n",
      "Item CF Cosine - Precision@10: 0.0643, Recall@10: 0.1262, MRR@10: 0.1821, RMSE: 2.6268\n",
      "Evaluating Content CF Cosine...\n",
      "Content CF Cosine - Precision@10: 0.0071, Recall@10: 0.0179, MRR@10: 0.0357, RMSE: 2.1500\n",
      "Evaluating Matrix Factorisation SVD...\n",
      "Matrix Factorisation SVD - Precision@10: 0.0357, Recall@10: 0.0353, MRR@10: 0.1786, RMSE: 2.0380\n",
      "Evaluating Matrix Factorisation NMF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Factorisation NMF - Precision@10: 0.0429, Recall@10: 0.0387, MRR@10: 0.1865, RMSE: 3.1826\n",
      "Evaluating Matrix Factorisation ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 150.46it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 92.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 143.93it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 144.11it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 154.68it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 151.32it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 147.19it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 146.88it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 157.88it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 157.84it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 151.80it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 152.82it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 154.95it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 151.21it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 156.13it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 156.52it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 156.95it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 155.95it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 161.53it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 153.54it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 156.14it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 152.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 147.82it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 155.82it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 154.47it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 154.35it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 157.36it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 162.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Factorisation ALS - Precision@10: 0.0286, Recall@10: 0.0238, MRR@10: 0.0476, RMSE: 3.7432\n",
      "Evaluating Hybrid CF & Avg Weighted...\n",
      "Hybrid CF & Avg Weighted - Precision@10: 0.1000, Recall@10: 0.1765, MRR@10: 0.4355, RMSE: 3.3867\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "results_10 = evaluate_models(df_train, df_test, models, N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dceaa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Top N Count...\n",
      "Top N Count - Precision@5: 0.0429, Recall@5: 0.0972, MRR@5: 0.1607, RMSE: 180.0356\n",
      "Evaluating Top N Likes...\n",
      "Top N Likes - Precision@5: 0.0571, Recall@5: 0.0903, MRR@5: 0.1905, RMSE: 142.0416\n",
      "Evaluating Average Rating...\n",
      "Average Rating - Precision@5: 0.0000, Recall@5: 0.0000, MRR@5: 0.0000, RMSE: nan\n",
      "Evaluating Average Rating Normalized...\n",
      "Average Rating Normalized - Precision@5: 0.0000, Recall@5: 0.0000, MRR@5: 0.0000, RMSE: nan\n",
      "Evaluating Average Rating Weighted...\n",
      "Average Rating Weighted - Precision@5: 0.0571, Recall@5: 0.0903, MRR@5: 0.1786, RMSE: 0.3446\n",
      "Evaluating User CF Pearson...\n",
      "User CF Pearson - Precision@5: 0.1000, Recall@5: 0.1079, MRR@5: 0.2381, RMSE: 1.7792\n",
      "Evaluating User CF Cosine...\n",
      "User CF Cosine - Precision@5: 0.1429, Recall@5: 0.1354, MRR@5: 0.3952, RMSE: 2.2955\n",
      "Evaluating Item CF Pearson...\n",
      "Item CF Pearson - Precision@5: 0.0143, Recall@5: 0.0102, MRR@5: 0.0143, RMSE: 3.0000\n",
      "Evaluating Item CF Cosine...\n",
      "Item CF Cosine - Precision@5: 0.0714, Recall@5: 0.0349, MRR@5: 0.1631, RMSE: 2.0869\n",
      "Evaluating Content CF Cosine...\n",
      "Content CF Cosine - Precision@5: 0.0143, Recall@5: 0.0179, MRR@5: 0.0357, RMSE: 2.1500\n",
      "Evaluating Matrix Factorisation SVD...\n",
      "Matrix Factorisation SVD - Precision@5: 0.0571, Recall@5: 0.0323, MRR@5: 0.1786, RMSE: 2.2786\n",
      "Evaluating Matrix Factorisation NMF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n",
      "/Users/timonweidemann/.pyenv/versions/3.10.15/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 500 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Factorisation NMF - Precision@5: 0.0571, Recall@5: 0.0218, MRR@5: 0.1786, RMSE: 2.4230\n",
      "Evaluating Matrix Factorisation ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 151.19it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 155.57it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 152.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 135.41it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 154.80it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 145.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 158.77it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 155.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 159.48it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 159.95it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 156.91it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 155.92it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 157.80it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 158.47it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 159.35it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 163.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 162.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 157.24it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 163.28it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 156.55it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 159.49it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 161.45it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 158.72it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 160.35it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 161.32it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 161.37it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 163.97it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 162.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Factorisation ALS - Precision@5: 0.0286, Recall@5: 0.0060, MRR@5: 0.0238, RMSE: 3.7765\n",
      "Evaluating Hybrid CF & Avg Weighted...\n",
      "Hybrid CF & Avg Weighted - Precision@5: 0.1833, Recall@5: 0.1557, MRR@5: 0.4236, RMSE: 3.3452\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "results_5 = evaluate_models(df_train, df_test, models, N=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e4d56",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce11ca9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
